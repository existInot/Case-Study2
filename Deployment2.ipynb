{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "#import helper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from tabulate import tabulate\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('tokenizerc.pickle', 'rb') as handle:\n",
    "    tknizer_c = pickle.load(handle)\n",
    "    # loading\n",
    "with open('tokenizeruc.pickle', 'rb') as handle:\n",
    "    tknizer_uc = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3008\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "vocab_size_uc=len(tknizer_uc.word_index.keys())\n",
    "print(vocab_size_uc)\n",
    "vocab_size_c=len(tknizer_c.word_index.keys())\n",
    "print(vocab_size_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "\n",
    "      super().__init__()\n",
    "      self.inp_vocab_size= inp_vocab_size\n",
    "      self.embedding_size = embedding_size\n",
    "      self.input_length = input_length\n",
    "      self.lstm_size = lstm_size\n",
    "      self.lstm_output = 0\n",
    "      self.lstm_state_h = 0\n",
    "      self.lstm_state_c = 0\n",
    "      self.embedding = Embedding(input_dim=self.inp_vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
    "                            mask_zero=True, name=\"embedding_layer_encoder\")\n",
    "      self.lstm = LSTM(self.lstm_size, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "      input_embedd= self.embedding(input_sequence)\n",
    "      self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n",
    "      return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "\n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      #check\n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "      '''\n",
    "      initial_hidden_state =tf.zeros(shape=[batch_size,self.lstm_size],dtype=tf.int32)\n",
    "      initial_cell_state =tf.zeros(shape=[batch_size,self.lstm_size],dtype=tf.int32)\n",
    "      return initial_hidden_state,initial_cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,scoring_function, att_units):\n",
    "    super().__init__()\n",
    "    self.scoring_function = scoring_function\n",
    "    self.att_units = att_units\n",
    "\n",
    "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "\n",
    "    if self.scoring_function=='dot':\n",
    "      # Intialize variables needed for Dot score function here\n",
    "      # encoder output\n",
    "      self.Dot = tf.keras.layers.Dot(axes=(1, 2))\n",
    "    if scoring_function == 'general':\n",
    "      self.W1= tf.keras.layers.Dense(att_units)\n",
    "      self.Dot = tf.keras.layers.Dot(axes=(1, 2))\n",
    "    elif scoring_function == 'concat':\n",
    "      # Intialize variables needed for Concat score function here\n",
    "        self.W2 = tf.keras.layers.Dense(att_units)\n",
    "        self.W3 = tf.keras.layers.Dense(att_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "  \n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    '''\n",
    "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "        Multiply the score function with your encoder_outputs to get the context vector.\n",
    "        Function returns context vector and attention weights(softmax - scores)\n",
    "    '''\n",
    "    if self.scoring_function == 'dot':\n",
    "        # Implement Dot score function here\n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state,1)\n",
    "        dhs = tf.transpose(decoder_hidden_state,(0,2,1))\n",
    "        #print(dhs.shape)\n",
    "        #print(encoder_output.shape)\n",
    "        x = self.Dot([dhs,encoder_output])\n",
    "        a = tf.nn.softmax(tf.transpose(x,(0,2,1)),axis=1)\n",
    "        c = a*encoder_output\n",
    "        c = tf.reduce_sum(c,axis = 1)\n",
    "        #print(c.shape,a.shape)\n",
    "        return c,a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    elif self.scoring_function == 'general':\n",
    "        # Implement General score function here\n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state,1)\n",
    "        wa = self.W1(encoder_output)\n",
    "        dhs = tf.transpose(decoder_hidden_state,(0,2,1))\n",
    "        x = self.Dot([dhs,wa])\n",
    "        \n",
    "        a = tf.nn.softmax(tf.transpose(x,(0,2,1)),axis=1)\n",
    "        c = a*encoder_output\n",
    "        c = tf.reduce_sum(c,axis = 1)\n",
    "        #print(c.shape,a.shape)\n",
    "        return c,a\n",
    "\n",
    "    elif self.scoring_function == 'concat':\n",
    "        # Implement General score function here\n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state,1)\n",
    "        e = self.V(tf.nn.tanh(self.W2(decoder_hidden_state)+self.W3(encoder_output)))\n",
    "        a = tf.nn.softmax(e,axis = 1)\n",
    "        c = a*encoder_output\n",
    "        c = tf.reduce_sum(c,axis = 1)\n",
    "        m = self.W2\n",
    "        #print(c.shape,a.shape)\n",
    "        #print(m.shape)\n",
    "        return c,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "    super().__init__()\n",
    "    self.tar_vocab_size = tar_vocab_size \n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.input_length =  input_length\n",
    "    self.dec_units = dec_units\n",
    "    self.score_fun = score_fun\n",
    "    self.att_units = att_units\n",
    "# Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "    self.lstm_output = 0\n",
    "    self.lstm_state_h = 0\n",
    "    self.lstm_state_c = 0\n",
    "    self.embedding = Embedding(input_dim=self.tar_vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                            mask_zero=True, name=\"embedding_layer_encoder\")\n",
    "    self.lstm = LSTM(att_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "    self.dense   = Dense(self.tar_vocab_size)\n",
    "    self.attention = Attention(scoring_function = self.score_fun,att_units = self.att_units)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "    '''\n",
    "        One step decoder mechanisim step by step:\n",
    "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "      C. Concat the context vector with the step A output\n",
    "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "    '''\n",
    "    \n",
    "    input_embedding= self.embedding(input_to_decoder)\n",
    "    context,similarities = self.attention(state_h,encoder_output)\n",
    "    #print(input_embedding.shape)\n",
    "    #print(context.shape)\n",
    "    concat = tf.keras.layers.Concatenate(axis = 2)([input_embedding,tf.expand_dims(context,1)])\n",
    "    decoder_output,*states = self.lstm(concat)\n",
    "    \n",
    "    #print(states[0].shape)\n",
    "    decoder_hidden,decoder_cell = states[0],states[1]\n",
    "\n",
    "    output =self.dense(decoder_output[:,:,:self.dec_units])\n",
    "    #print(decoder_hidden.shape)\n",
    "    return tf.squeeze(output,axis = 1),decoder_hidden,decoder_cell,similarities,context\n",
    "#context_vector = 32116\n",
    "#i2d_output = 32112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "                \n",
    "      super().__init__()\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "      self.out_vocab_size = out_vocab_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "\n",
    "      self.input_length = input_length\n",
    "      self.dec_units = dec_units\n",
    "      self.score_fun = score_fun\n",
    "      self.att_units = att_units\n",
    "      self.onestepdecoder=One_Step_Decoder(self.out_vocab_size, self.embedding_dim, self.input_length, self.dec_units ,self.score_fun ,self.att_units)  \n",
    "\n",
    "        \n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
    "\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "        \n",
    "        #Iterate till the length of the decoder input\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            # Store the output in tensorarray\n",
    "        # Return the tensor array\n",
    "        all_outputs = tf.TensorArray(tf.float32, size=40, name = 'output_arrays',dynamic_size = True,clear_after_read = False)\n",
    "        for i in range(40):\n",
    "          output,decoder_hidden_state ,decoder_cell_state,attention_weights,context_vector = self.onestepdecoder(input_to_decoder[:,i:i+1],encoder_output,decoder_hidden_state,decoder_cell_state)#sequence is correct\n",
    "          all_outputs = all_outputs.write(i,output)\n",
    "\n",
    "       # all_outputs.mark_used()\n",
    "\n",
    "\n",
    "        all_outputs = tf.transpose(all_outputs.stack(),[1,0,2])\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self,encoder_inputs_length,decoder_inputs_length,output_vocab_size,batch_size):\n",
    "    #Intialize objects from encoder decoder\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size = vocab_size_c+2,embedding_size = 100,lstm_size = 512,input_length = encoder_inputs_length)\n",
    "        self.decoder = Decoder(out_vocab_size = vocab_size_uc+2,embedding_dim = 100,input_length = decoder_inputs_length ,dec_units = 512,score_fun = 'concat',att_units = 512)\n",
    "        self.dense   = Dense(output_vocab_size, activation='softmax')\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = Embedding(input_dim=vocab_size_c+1, output_dim=100, input_length=40,\n",
    "                               mask_zero=True, name=\"embedding_layer_encoder\")\n",
    "\n",
    "  \n",
    "    def call(self,data):\n",
    "        #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "        # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "        # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "        # return the decoder output\n",
    "        input,output = data[0],data[1]\n",
    "        initial_state = self.encoder.initialize_states(self.batch_size)\n",
    "        input_embedd= input\n",
    "        encoder_output,encoder_h,encoder_c = self.encoder(input_embedd,initial_state)\n",
    "        decoder_output = self.decoder(output,encoder_output,encoder_h,encoder_c)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\" Custom loss function that will not consider the loss for padded zeros.\n",
    "    why are we using this, can't we use simple sparse categorical crossentropy?\n",
    "    Yes, you can use simple sparse categorical crossentropy as loss like we did in task-1. But in this loss function we are ignoring the loss\n",
    "    for the padded zeros. i.e when the input is zero then we donot need to worry what the output is. This padded zeros are added from our end\n",
    "    during preprocessing to make equal length for all the sentences.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = encoder_decoder(encoder_inputs_length=40,decoder_inputs_length=40,output_vocab_size=vocab_size_uc,batch_size =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.built = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x17dde2ad550>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('my_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "\n",
    "  '''\n",
    "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "  \n",
    "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "  D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         Save the attention weights\n",
    "         And get the word using the tokenizer(word index) and then store it in a string.\n",
    "  E. Call plot_attention(#params)\n",
    "  F. Return the predicted sentence\n",
    "  '''\n",
    "  attention_plot = np.zeros((40,40))\n",
    "  input_sequences = tknizer_c.texts_to_sequences([input_sentence])\n",
    "  encoder_sequence = pad_sequences(input_sequences, maxlen = 40, dtype='int32', padding='post')\n",
    "  encoder_output,state_h,state_c= model.layers[0](encoder_sequence,model.layers[0].initialize_states(1))\n",
    "  dec_input = tknizer_uc.word_index['<start>']\n",
    "  dec_input = (np.array(dec_input)).reshape(1,1)\n",
    "  states = [state_h,state_c]\n",
    "  j = 0\n",
    "  final = []\n",
    "  while j!=40:\n",
    "    predicted_out,state_h,state_c,attention_weights,context_vector=model.layers[1].onestepdecoder(dec_input,encoder_output,state_h,state_c)\n",
    "    #att_w.append(state_c)\n",
    "    \n",
    "    attention_weights = tf.reshape(attention_weights,(-1,))\n",
    "    attention_plot[j] = attention_weights.numpy()\n",
    "    #______________________________________\n",
    "\n",
    "    output = predicted_out\n",
    "    states = [state_h,state_c] #automatically get updated as above\n",
    "    output= tf.argmax(output[0]).numpy()\n",
    "    #print(output)\n",
    "    b = get_key(output)\n",
    "    if b == '<end>':\n",
    "      break\n",
    "    else:\n",
    "      final.append(get_key(output))\n",
    "      dec_input =tf.expand_dims([output], 0)\n",
    "      j+=1\n",
    " # plot_attention(att_w,input_sentence,final)\n",
    "  \n",
    "  return ' '.join(final),input_sentence,attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val):\n",
    "    for key, value in tknizer_uc.word_index.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontractions(phrase):\n",
    "    \"\"\"decontracted takes text and convert contractions into natural form.\n",
    "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "\n",
    "    return phrase\n",
    "\n",
    "def preprocess(text):\n",
    "    # convert all the text into lower letters\n",
    "    # use this function to remove the contractions: https://gist.github.com/anandborad/d410a49a493b56dace4f814ab5325bbd\n",
    "    # remove all the spacial characters: except space ' '\n",
    "    text = text.lower()\n",
    "    text = decontractions(text)\n",
    "    text = re.sub('[$)\\?\"’.°!;\\'€%:,(/]', '', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_c(text):\n",
    "    # convert all the text into lower letters\n",
    "    # remove the words betweent brakets ()\n",
    "    # remove these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
    "    # replace these spl characters with space: '\\u200b', '\\xa0', '-', '/'\n",
    "    # we have found these characters after observing the data points, feel free to explore more and see if you can do find more\n",
    "    # you are free to do more proprocessing\n",
    "    # note that the model will learn better with better preprocessed data \n",
    "    \n",
    "    text = text.lower()\n",
    "    text = decontractions(text)\n",
    "    text = re.sub('[$)\\?\"’.°!;\\'€%:,(/]', '', text)\n",
    "    text = re.sub('\\u200b', ' ', text)\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final(x):\n",
    "  z = []\n",
    "  dfx  = pd.read_csv(x)\n",
    "  dfx = dfx.drop(\"Unnamed: 0\",axis = 1)\n",
    "  dfx = dfx.drop(\"Unnamed: 0.1\",axis = 1)\n",
    " \n",
    "  c  = dfx.columns[0]\n",
    "  dfx[\"c_l\"] =  dfx[c].str.split().apply(len)\n",
    "  dfx = dfx[dfx['c_l'] < 40]\n",
    "  dfx = dfx.drop(['c_l'],axis = 1)\n",
    "  dfx[c] = dfx[c].apply(preprocess_c)\n",
    "  for j in range(len(dfx)):\n",
    "        f,i,at  =  predict(dfx.iloc[j][0])\n",
    "        z.append(f)\n",
    "  return z\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [15/Feb/2022 11:01:33] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [15/Feb/2022 11:01:51] \"\u001b[37mPOST /home HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [15/Feb/2022 11:04:12] \"\u001b[35m\u001b[1mPOST /home HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary modules\n",
    "from flask import Flask, render_template, request\n",
    "import pickle\n",
    "import numpy as np\n",
    "# Loading the Models\n",
    "\n",
    "\n",
    "# Instantiate app object \n",
    "app = Flask(__name__,template_folder='templates')\n",
    "# Setting up app routes\n",
    "@app.route('/')\n",
    "def man(): \n",
    "    return render_template('Home.html')\n",
    "@app.route('/home', methods=['POST'])\n",
    "def home():\n",
    "    feature1 =(request.form['filename'])\n",
    "    predicted_values = Final(feature1)\n",
    "    return render_template('Home.html', prediction_text= (predicted_values))\n",
    "# Flask server\n",
    "if __name__==\"__main__\":\n",
    "    app.run(debug=True,use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
